{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom tqdm.auto import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Generate Binary mask to find sub-text within text\ndef get_mask(text, subtext):\n    loc = text.find(subtext)\n    prefix = text[:loc]\n    start = len(prefix.split())\n    end   = start + len(subtext.split())\n    mask = [0]*len(text.split())\n    mask[start:end] = [1]*(end-start)\n    return mask\n#Get Data\nclass TweetaSet(Dataset):\n    def __init__(self, filename, tokenizer, max_len, frac=1.0):\n        self.df = pd.read_csv(filename,header=0).dropna().sample(frac=frac)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, item):\n        textID, context, answer, question = self.df.iloc[item]\n        #Pre-encode context to get attention mask (masks out unknown words)\n        context_enc = self.tokenizer.encode_plus(\n            context,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        #get binary mask and bitwise and with attention mask to get rid of unknown words\n        mask = get_mask(context, answer)\n        mask += [0]*max(0,self.max_len-len(mask)) \n        mask = torch.Tensor(mask).int() & context_enc[\"attention_mask\"].int()\n        start = torch.nonzero(mask)[0][0]\n        end = start + torch.sum(mask)\n        \n        #encode question and context together\n        encoding = self.tokenizer.encode_plus(\n            question,context,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'start_positions' : start,\n            'end_positions'   : end\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n#standard huggingface finetuning training loop (see website for more details)\ndef train(filename, max_len, batch_size, epochs, frac=1.0, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    tokenizer = DistilBertTokenizer.from_pretrained('/kaggle/input/distilbert-cased-for-qa')\n    model = DistilBertForQuestionAnswering.from_pretrained('/kaggle/input/distilbert-cased-for-qa').to(device)\n    for param in model.base_model.parameters():\n        param.requires_grad = False\n    model.train()\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n    scheduler = get_linear_schedule_with_warmup(optimizer, 5 if epochs >= 10 else 0, epochs)\n    ds = TweetaSet(filename, tokenizer, max_len, frac=frac)\n    dl = DataLoader(ds, batch_size=batch_size)\n    for _ in range(epochs):\n        for batch in tqdm(dl):\n            batch = {k:v.to(device) for k,v in batch.items()}\n            optimizer.zero_grad()\n            outputs = model(**batch)\n            loss,_,_ = outputs\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n    model.eval()\n    return model,tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model,tokenizer = train(\"/kaggle/input/tweet-sentiment-extraction/train.csv\", 35, 100, 50, frac=0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get test data\nclass TweetaTestSet(Dataset):\n    def __init__(self, filename, tokenizer, max_len, frac=1.0):\n        self.df = pd.read_csv(filename).dropna().sample(frac=frac)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, item):\n        textID, context, question = self.df.iloc[item]\n        encoded = self.tokenizer.encode_plus(\n            question,context,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'id': textID,\n            'context': context,\n            'input_ids': encoded['input_ids'].flatten(),\n            'attention_mask': encoded['attention_mask'].flatten()\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#generate output data\nimport torch.nn.functional as F\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nds = TweetaTestSet(\"/kaggle/input/tweet-sentiment-extraction/test.csv\", tokenizer, 35)\ndf = pd.DataFrame(columns=[\"textID\", \"selected_text\"])\nfor batch in DataLoader(ds, batch_size=100):\n    start_logits, end_logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n    start = torch.argmax(F.log_softmax(start_logits, dim=1),dim=1)\n    end   = torch.argmax(F.log_softmax(end_logits, dim=1),dim=1)\n    print((start,end,[t[s:e] for t,s,e in zip(batch[\"context\"], start.int(), end.int())]))\n    df2 = pd.DataFrame({\n        \"textID\": batch[\"id\"],\n        \"selected_text\": [t[s:e] for t,s,e in zip(batch[\"context\"], start.int(), end.int())]\n    })\n    df = df.append(df2)\ndf.to_csv(\"/kaggle/working/submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}