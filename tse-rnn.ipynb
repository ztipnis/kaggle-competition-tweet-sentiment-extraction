{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertModel\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom tqdm.auto import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#find the starting and ending positions of the masked data\ndef get_mask(text, subtext):\n    start = text.find(subtext)\n    end = start + len(subtext)\n    return {\"start\": len(text[:start].split()), \"end\": len(text[:end].split())}\n#see DistilBert GAN for details\nclass TweetaSet(Dataset):\n    def __init__(self, filename, tokenizer, max_len, frac=1.0):\n        self.df = pd.read_csv(filename).dropna().sample(frac=frac)\n        self.sentiment_to_idx = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, item):\n        textID, text, selected_text, sentiment = self.df.iloc[item]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n\n        mask = get_mask(text, selected_text)\n        return {\n            'tweet': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'output_mask': mask,\n            'sentiment':self.sentiment_to_idx[sentiment]\n        }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#See Report for Details\nclass BERTRNNFineTuner(nn.Module):\n    def __init__(self, max_len, hidden):\n        super(BERTRNNFineTuner,self).__init__()\n        self.bert      = DistilBertModel.from_pretrained('/kaggle/input/distillbert-base-cased/hf-distillbert-base-cased', output_attentions=True)\n        self.recurrent = nn.LSTM(self.bert.config.hidden_size,128,3)\n        self.dense     = nn.Linear(128,128)\n        self.hidden    = hidden\n        nn.init.normal_(hidden[0])\n        nn.init.normal_(hidden[1])\n        self.relu      = nn.ReLU()\n        self.lin       = nn.Linear(129,2)\n        self.max_len = max_len\n        for name, param in self.recurrent.named_parameters():\n            if 'bias' in name:\n                nn.init.constant_(param, 0.0)\n            elif 'weight' in name:\n                nn.init.xavier_normal_(param)\n    def forward(self, input_ids, attn_mask, sentiment):\n        x = self.bert(input_ids=input_ids, attention_mask=attn_mask)[0]\n        x, hidden = self.recurrent(x, self.hidden)\n#         self.hidden = tuple([each.data for each in hidden])\n        x = self.dense(x)\n        y = sentiment.view(-1,1,1).repeat(1,self.max_len,1)\n        x = torch.cat((x,y), dim=2)\n        x = self.relu(x)\n        x = self.lin(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Weight the loss functions of two different outcomes, start always approaches 0 so end is a bigger target\nclass BiCELoss:\n    def __init__(self):\n        self.loss_fn = nn.CrossEntropyLoss()\n    def __call__(self, logits, start_act, end_act):\n        start,end = logits.permute(2,0,1)\n        return 0.8*self.loss_fn(start, start_act) + 1.2*self.loss_fn(end, end_act)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for testing accuracy on test data\n#calculate binary intersection over union\nclass JaccardAccuracyMeasure:\n    def __call__(self, str1, str2):\n        s1 = set(str1)\n        s2 = set(str2)\n        return len(s1.intersection(s2))/float(len(s1.union(s2))+1e-300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n#Standard training loop for pytorch.\ndef train(epochs, batch_size=100, max_len=35, frac=1.0, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    tok = DistilBertTokenizer.from_pretrained('/kaggle/input/distillbert-base-cased/hf-distillbert-base-cased')\n    ds = TweetaSet(\"/kaggle/input/tweet-sentiment-extraction/train.csv\", tok, max_len, frac)\n    dl = DataLoader(ds, batch_size=100, pin_memory=(device==\"cuda\"))\n    model = BERTRNNFineTuner(max_len, (torch.zeros((3,max_len,128)).to(device),torch.zeros((3,max_len,128)).to(device))).to(device)\n    loss_fn = BiCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n    acc = JaccardAccuracyMeasure()\n    for _ in range(epochs):\n        a = random.randrange(50)\n        for i,batch in enumerate(tqdm(dl)):\n            optimizer.zero_grad()\n            inputs = (\n                batch[\"input_ids\"].to(device),\n                batch[\"attention_mask\"].to(device),\n                batch[\"sentiment\"].to(device)\n            )\n            logits = model(*inputs)\n            loss = loss_fn(logits,batch[\"output_mask\"][\"start\"].to(device),batch[\"output_mask\"][\"end\"].to(device))\n            start,end = torch.argmax(F.log_softmax(logits, dim=1),dim=1).squeeze().permute(1,0)\n            loss.backward()\n            if i%100 == 99:\n                tqdm.write(\"Loss: {:0.5f}\".format(loss))\n            if i%50 == a:\n                str2s = [t.split()[(min(s,len(t.split()))):(min(e,len(t.split())))] for t,s,e in zip(batch[\"tweet\"],start,end)]\n                str1s = [b.split()[s:e] for b,s,e in zip(batch[\"tweet\"], batch[\"output_mask\"][\"start\"], batch[\"output_mask\"][\"end\"])]\n                accs = [acc(*z) for z in zip(str1s,str2s)]\n                tqdm.write(\"Accuracy: {:0.5f}\".format(sum(accs)/float(len(accs))))\n            optimizer.step()\n    model.eval()\n    return tok,model\n\n\n            \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer,model = train(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get Test Data\nclass TweetaTestSet(Dataset):\n    def __init__(self, filename, tokenizer, max_len, frac=1.0):\n        self.df = pd.read_csv(filename).dropna().sample(frac=frac)\n        self.sentiment_to_idx = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, item):\n        textID, text, sentiment = self.df.iloc[item]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'id': textID,\n            'tweet': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'sentiment':self.sentiment_to_idx[sentiment]\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate kaggle submission\ndl = DataLoader(TweetaTestSet(\"/kaggle/input/tweet-sentiment-extraction/test.csv\", tokenizer, 35), batch_size=10)\ndf = pd.DataFrame(columns=[\"textID\", \"selected_text\"])\nmodel=model.to('cpu')\nfor batch in dl:\n    inputs = (\n                batch[\"input_ids\"],\n                batch[\"attention_mask\"],\n                batch[\"sentiment\"]\n            )\n    logits = model(*inputs)\n    start,end = torch.argmax(F.log_softmax(logits, dim=1),dim=1).squeeze().permute(1,0)\n    text = [t.split()[(min(s,len(t.split()))):(min(e,len(t.split())))] for t,s,e in zip(batch[\"tweet\"],start,end)]\n    df2 = pd.DataFrame(batch[\"textID\"], text, colums = [\"textID, selected_text\"])\n    df = df.append(df2)\ndf.to_csv(\"/kaggle/working/submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}