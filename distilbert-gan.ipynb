{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n#use pre-downloaded huggingface transformers as the competition does not allow internet access\nos.environ['TRANSFORMERS_CACHE'] = \"/kaggle/input/hf-cache/hf\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertModel, AdamW\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a binary mask by finding the substring within the string\ndef get_mask(text, subtext):\n    #split strings into words\n    text = text.split()\n    subtext = subtext.split()\n    \n    temp = []\n    while len(text) > 0:\n        #keep adding zeros to mask until found matching word\n        if text[0] != subtext[0]:\n            temp.append(0)\n            text = text[1:]\n        else:\n            flag = False\n            #compare each word in the substring to the following word. Set flag if they do not match\n            for i in range(1,min(len(text),len(subtext))):\n                if(text[i] != subtext[i]):\n                    flag = True\n                    break\n            #if flag is not set, the entire substring was found, otherwise\n            # add zero to temp and start search again with the next word in the string\n            if flag:\n                temp.append(0)\n                text = text[1:]\n            else:\n                #there is only one occurance of the masked substring,\n                #so once we find it, fill in the rest and return\n                temp += [1] * len(subtext) + [0] * (len(text)-len(subtext))\n                return temp\n    #since we know the substring it always in the string,\n    #this will never be hit, but just in case retain the\n    #ability to return a 0-only mask\n    return temp\n    \n#Pytorch DataSet\nclass TweetaSet(Dataset):\n    def __init__(self, filename, tokenizer, max_len, frac=1.0):\n        #Pandas Dataframe, if only using part of data sample it as such\n        self.df = pd.read_csv(filename).dropna().sample(frac=frac)\n        #Convert sentiment words into numerical values\n        self.sentiment_to_idx = {v:k for k,v in enumerate(self.df.sentiment.unique())}\n        #load in the huggingface tokenizer\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, item):\n        textID, text, selected_text, sentiment = self.df.iloc[item]\n        #create encoding and mask features\n        encoding = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n\n        mask = get_mask(text, selected_text)\n        mask = mask + [0] * (self.max_len - len(mask))\n        return {\n            'tweet': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'output_mask': torch.tensor(mask, dtype=torch.int),\n            'sentiment':self.sentiment_to_idx[sentiment]\n        }\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#See report for architecture explanations\nclass MaskDiscriminator(nn.Module):\n    def __init__(self, bert, mask_size, dropout_p=0.3):\n        super(MaskDiscriminator, self).__init__()\n        self.bert = bert\n        self.norm_1 = nn.BatchNorm1d(mask_size)\n        self.mask = nn.Linear(1+self.bert.config.hidden_size, 1)\n        self.drop = nn.Dropout(p=dropout_p)\n        self.rect = nn.ReLU()\n        self.norm_2 = nn.BatchNorm1d(mask_size*2)\n        self.mask_2  = nn.Linear(mask_size*2, 1)\n        self.out = nn.Sigmoid()\n    def forward(self, input_ids, attention_mask, mask, sentiment):\n        x = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]\n        x_1 = sentiment.unsqueeze(1).repeat(1,x.size(1)).unsqueeze(2)\n        x = self.norm_1(x)\n        x = torch.cat((x_1,x), dim=2)\n        x = self.mask(x).squeeze()\n        x = self.rect(x)\n        x = self.drop(x)\n        x = torch.cat((x,mask),dim=1)\n        x = self.norm_2(x)\n        x = self.mask_2(x)\n        x = self.out(x)\n        return x\nclass MaskGenerator(nn.Module):\n    def __init__(self, bert, mask_size, batch_size, dropout_p=0.3):\n        super(MaskGenerator, self).__init__()\n        self.bert = bert\n        self.dense_1 = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n        self.norm_1 = nn.BatchNorm1d(mask_size)\n        self.relu = nn.ReLU()\n        self.activation = nn.Sigmoid()\n        self.rnn = nn.LSTM(1+self.bert.config.hidden_size, 128, 1)\n        self.norm_2  = nn.BatchNorm1d(mask_size)\n        self.sparse = nn.Linear(128, 1)\n        self.dense_2 = nn.Linear(mask_size, mask_size)\n        self.hidden = (\n            torch.zeros((1,mask_size, 128)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n            torch.zeros((1,mask_size, 128)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n    def forward(self, input_ids, attention_mask, sentiment):\n        x = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]\n        x = self.norm_1(x)\n        x = self.dense_1(x)\n        x = self.relu(x)\n        x_1 = sentiment.unsqueeze(1).repeat(1,x.size(1)).unsqueeze(2)\n        x = torch.cat((x_1,x), dim=2)\n        x,self.hidden = self.rnn(x, self.hidden)\n        self.hidden = tuple([each.data for each in self.hidden])\n        x = self.sparse(x).squeeze()\n        x = self.norm_2(x)\n        x = self.dense_2(x)\n        x = self.activation(x)\n        return x\n    \n#share one bert model amongst both\ndef generate_models(mask_size, batch_size):\n    bert_shared = DistilBertModel.from_pretrained('distilbert-base-cased', local_files_only=True)\n    return nn.DataParallel(MaskDiscriminator(bert_shared, mask_size)), nn.DataParallel(MaskGenerator(bert_shared, mask_size, batch_size)), bert_shared.config\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate the intersection and union of two masks.\n#As a loss function, one vector will be a probabilty \n# array but that still gets the desired result while\n# eliminating the discrete-step problem\nclass JaccardLoss(object):\n    def __call__(self,pred, actual):\n        intersection =(actual*pred)\n        union = ((pred+actual)- intersection)\n        intersection= torch.sum(intersection, dim=-1)**2\n        union=torch.sum(union, dim=-1)**2\n        return 1-(torch.mean((intersection + 1e-6)/(union+1e-6)))**(1/2)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.auto import tqdm\ndef train(epochs, dataset, batch_size=16, max_size=40, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    torch.autograd.set_detect_anomaly(True)\n    device = torch.device(device)\n    #Models\n    discriminator,generator, config = generate_models(max_size, batch_size)\n    discriminator = discriminator.to(device)\n    generator.to(device)\n    #Optimizers\n    generator_optimizer = AdamW(generator.parameters(), lr=0.01)\n    discriminator_optimizer = AdamW(discriminator.parameters(), lr=0.001)\n    \n    #Loss\n    classify_loss = JaccardLoss()\n    masking_loss = JaccardLoss()\n    \n    #Data\n    data = DataLoader(dataset, batch_size=batch_size, num_workers=4)\n    \n    #pre-train generator with Adadelta and high learning rate\n    pretrain_optim = optim.Adadelta(generator.parameters(), lr=10.0)\n    print(\"Pre-training generator\")\n    for _ in range(epochs):\n        running_loss = 0.0\n        ctr = 1.0\n        with tqdm(data) as _data:\n            for batch in _data:\n                pretrain_optim.zero_grad()\n                mask = generator(batch['input_ids'].to(device), batch['attention_mask'].to(device),batch['sentiment'].to(device))\n                loss = masking_loss(mask,batch['output_mask'].to(device))\n                running_loss += loss\n                loss.backward()\n                pretrain_optim.step()\n                _data.set_description(\"Loss: {:0.5f},\".format(loss))\n                torch.cuda.empty_cache()\n                if(ctr % 10 == 9):\n                    tqdm.write(\"Running Pretrain loss: \" + str(float(running_loss / (ctr))))\n                ctr += 1\n    print(\"Diversifying\")\n    #Diversify with Adversarial network\n    for _ in range(epochs):\n        running_generator_loss = 0.0\n        running_descrim_loss = 0.0\n        ctr = 1\n        with tqdm(data) as _data:\n            for batch in _data:\n                generator_optimizer.zero_grad()\n\n                gen_mask_using_true = generator(batch['input_ids'].to(device), batch['attention_mask'].to(device),batch['sentiment'].to(device))\n\n                noise = (torch.abs(torch.rand(batch['input_ids'].size(),layout=batch['input_ids'].layout)) * (config.vocab_size-1)).long().to(device)\n\n                gen_attn_mask = torch.randint(0,2,batch['attention_mask'].size()).to(device)\n                gen_sentiments = torch.randint(0,3,batch['sentiment'].size()).to(device)\n                generated_mask = generator(input_ids=noise, attention_mask=gen_attn_mask,sentiment=gen_sentiments)\n                generator_discriminator_out = discriminator(noise, gen_attn_mask, (generated_mask>0.5), gen_sentiments)\n                gen_masking_loss = masking_loss(gen_mask_using_true, batch['output_mask'].to(device))\n                gen_class_loss   = classify_loss(generator_discriminator_out,torch.ones_like(generator_discriminator_out)) \n                generator_loss = (4*gen_masking_loss + gen_class_loss)/3\n                _data.set_description(\"Loss: {:0.5f}, {:0.5f}, {:0.5f}\".format(gen_masking_loss, gen_class_loss, generator_loss))\n                generator_loss.backward()\n                running_generator_loss += generator_loss\n                generator_optimizer.step()\n\n                true_data = (batch['input_ids'].to(device), batch['attention_mask'].to(device),batch['output_mask'].to(device), batch['sentiment'].to(device))\n\n\n                discriminator_optimizer.zero_grad()\n                true_discriminator_out = discriminator(*true_data)\n                true_discriminator_loss = classify_loss(true_discriminator_out, torch.ones_like(true_discriminator_out))\n\n                generator_discriminator_out = discriminator(noise, gen_attn_mask, generated_mask.detach(), gen_sentiments)\n                generator_discriminator_loss = classify_loss(generator_discriminator_out, torch.zeros_like(generator_discriminator_out))\n                discriminator_loss = (2*true_discriminator_loss + generator_discriminator_loss) / 3\n                discriminator_loss.backward()\n                running_descrim_loss += discriminator_loss\n                discriminator_optimizer.step()\n                torch.cuda.empty_cache()\n                if(ctr % 10 == 9):\n                    tqdm.write(\"Running generator loss: \" + str(float(running_generator_loss / (2*ctr))))\n                    tqdm.write(\"Running descriminator loss: \" + str(float(running_descrim_loss / (ctr))))\n                ctr += 1\n                \n    generator.eval()\n    return generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a class to use the trained model\nimport numpy as np\nclass Masker(object):\n    def __init__(self, training_data_path, batch_size=16, max_len=100, epochs=10, pretrained_weights=None, frac=1.0):\n        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased', local_files_only=True)\n        ds = TweetaSet(training_data_path, self.tokenizer, max_len, frac=frac)\n        self.sentiment_to_ix = ds.sentiment_to_idx\n        if(pretrained_weights == None):\n            self.generator = train(epochs, ds , max_size=max_len, batch_size=batch_size)\n        else:\n            bert = DistilBertModel.from_pretrained('distilbert-base-cased', local_files_only=True)\n            self.generator = nn.DataParallel(MaskGenerator(bert,max_len))\n            self.generator.load_state_dict(torch.load(pretrained_weights, map_location=torch.device('cpu')))\n        self.max_len = max_len\n        \n    def __call__(self, unmasked_strings, sentiments):\n        encodings = [self.tokenizer.encode_plus(\n            unmasked_string,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt'\n        ) for unmasked_string in unmasked_strings]\n        encoding = {\n            'input_ids':torch.cat([i['input_ids'] for i in encodings]),\n            'attention_mask':torch.cat([i['attention_mask'] for i in encodings]),\n        }\n        mask_input = (\n            encoding['input_ids'],\n            encoding['attention_mask'],\n            torch.Tensor([self.sentiment_to_ix[sentiment] for sentiment in sentiments])\n        )\n        masks = self.generator(*mask_input)\n        m = masks.cpu().detach().numpy()\n        print(m)\n        print(m>0.5)\n        mask_string_pairs = zip(masks,unmasked_strings)\n        \n        return [\" \".join(v for k,v in zip((mask),unmasked_string.split()) if k>=0.5) for mask,unmasked_string in mask_string_pairs]\n    def save(self, path):\n        torch.save(self.generator.state_dict(), path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mask the data\nmask = Masker(\"/kaggle/input/tweet-sentiment-extraction/train.csv\", batch_size=100, epochs=5, max_len=35)\n#save the gradient weights if desired for re-use later\n# mask.save(\"/kaggle/working/generator_state.dict\")\nmask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#generate submission\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\nguess = []\nfor i in range(0,len(test),100):\n    r = slice(i,min(i+100, len(test)))\n    guess += mask(test[\"text\"][r], test[\"sentiment\"][r])\ntest['selected_text'] = guess\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#write to file\ntest[[\"textID\",\"selected_text\"]].to_csv(\"/kaggle/working/submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}